# Some RAG-related materials

## Prompt
Prompt is everywhere
- [Prompt Engineering Guide](https://www.promptingguide.ai/zh/introduction/tips)
- [Microsoft Design Prompt](https://blog.aixcopilot.com/microsoft-official-prompt-tutorial-advanced-prompt-design-and-engineering)

## Understanding RAG
-  [Large Language Models and Search](https://weaviate.io/blog/llms-and-search)
-  [ACL 2023 Tutorial: Retrieval-based Language Models and Applications](https://acl2023-retrieval-lm.github.io/)

## How to use RAG?
-  [A Guide on 12 Tuning Strategies for Production-Ready RAG Applications](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439#a5e2)
-  [LlamaIndex talk](./LlamaIndex_Talk.pdf)
-  [Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation](https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)

## Some Papers
### Published
- **Facebook Introduces the concept of RAG:** [Retrieval-augmented generation for knowledge-intensive nlp tasks (NIPS2020)](https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)
- **HyDE**: [Precise Zero-Shot Dense Retrieval without Relevance Labels (ACL2023)](https://arxiv.org/pdf/2212.10496.pdf)
- [REALM: Retrieval-Augmented Language Model Pre-Training (ICML2020)]()

- **may be useful:**[RA-DIT: Retrieval-Augmented Dual Instruction Tuning (ICLR2024)](https://arxiv.org/pdf/2310.01352.pdf)
- **Explode on Twitter：**[Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection(ICLR2024高分6888)](https://arxiv.org/pdf/2310.11511.pdf)
- **Interesting, may be useful:**[Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory (NIPS2023)](https://arxiv.org/pdf/2305.02437.pdf)
- [Understanding Retrieval Augmentation for Long-Form Question Answering (ICLR2024)](https://arxiv.org/pdf/2310.12150.pdf)
- [Large Language Models Can Be Easily Distracted by Irrelevant Context (ICML2023)](https://arxiv.org/pdf/2302.00093.pdf)
- [Making Retrieval-Augmented Language Models Robust to Irrelevant Context (ICLR2024)](https://arxiv.org/pdf/2310.01558.pdf)

### Preprint
- [Retrieval-Generation Synergy Augmented Large Language Models](https://arxiv.org/pdf/2310.05149.pdf)
