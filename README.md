# Large Language Model material

This repository aims to record valuable, useful, and interesting LLM-related materials. 

There are already tons of materials on LLM. To keep from getting lost in them, we built this repository. **Rather than including all available materials, we will focus on curating a collection of the most valuable, useful, and interesting materials.** We read and evaluate all materials in person, and since this takes a long time, we also include the materials that look good for future reading. If some materials you think are good aren't included, PULL A REQUEST! If you find this useful or want to follow our progress, we look forward to your ‚≠ê!

‚úÖ indicates that we have read a material and believe it is worth preserving

üö© indicates that we think a material is excellent and probably worth reading

üî• indicates that a material is well-known in its field or popular now



---

## Contents
- [For Readers](#forreaders)
- [LLM Overview](#overview)
- [Prompt](#prompt)
  - [How to design a prompt](#How-to-design-a-prompt)
- [RAG](#rag)
  - [Understanding RAG](#Understanding-RAG)
  - [How to use RAG](#How-to-use-RAG)
  - [Papers](#Papers)
- [Fine-tune](#finetune)
  - [Papers](#Papers)
- [Agent](#agent)
  - [Papers](#Papers)


# For Readers <a name="forreaders"></a>
We will gradually expand the contents of this repository, which is an ongoing long-term endeavor.

## 2023.12.18 Announcement
We'll be focusing on updating RAG-related materials for 1 to 2 weeks

We will improve other aspects of the repository within 1 to 2 months
# Large Language Models Overview üöÄ <a name="overview"></a>
## Fine-tune V.S. RAG
- [Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs](https://arxiv.org/pdf/2312.05934.pdf)
- [(Video) Openai Talk: A Survey of Techniques for Maximizing LLM Performance](https://www.youtube.com/watch?v=ahnGLM-RC1Y&list=PLzgksPT5cGTXoNjSfbmDKsSxMbXJ4WX_1&index=1&t=1s)  ‚úÖüö©

  **A good talk that describes the specific steps of the LLM development process**

# Prompt üöÄ<a name="prompt"></a>
## How to design a prompt
- [(in Chinese) Prompt Engineering Guide](https://www.promptingguide.ai/zh/introduction/tips)
- [(in Chinese) Microsoft Design Prompt](https://blog.aixcopilot.com/microsoft-official-prompt-tutorial-advanced-prompt-design-and-engineering)


---
# RAG üöÄ<a name="rag"></a>

## Understanding RAG
-  [Large Language Models and Search](https://weaviate.io/blog/llms-and-search)
-  [ACL 2023 Tutorial: Retrieval-based Language Models and Applications](https://acl2023-retrieval-lm.github.io/)

## How to use RAG
-  [A Guide on 12 Tuning Strategies for Production-Ready RAG Applications](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439#a5e2)
-  [LlamaIndex talk](materials/RAG/LlamaIndex_Talk.pdf)
-  [Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation](https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)

## Papers
- üî•**Facebook Introduces the concept of RAG:** [Retrieval-augmented generation for knowledge-intensive nlp tasks (NIPS2020)](https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)
- üî•**HyDE**: [Precise Zero-Shot Dense Retrieval without Relevance Labels (ACL2023)](https://arxiv.org/pdf/2212.10496.pdf)
- [REALM: Retrieval-Augmented Language Model Pre-Training (ICML2020)]()
- **may be useful:**[RA-DIT: Retrieval-Augmented Dual Instruction Tuning (ICLR2024)](https://arxiv.org/pdf/2310.01352.pdf)
- üî•**Explode on TwitterÔºö**[Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection(ICLR2024È´òÂàÜ6888)](https://arxiv.org/pdf/2310.11511.pdf)
- **Interesting, may be useful:**[Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory (NIPS2023)](https://arxiv.org/pdf/2305.02437.pdf)
- [Understanding Retrieval Augmentation for Long-Form Question Answering (ICLR2024)](https://arxiv.org/pdf/2310.12150.pdf)
- [Large Language Models Can Be Easily Distracted by Irrelevant Context (ICML2023)](https://arxiv.org/pdf/2302.00093.pdf)
- [Making Retrieval-Augmented Language Models Robust to Irrelevant Context (ICLR2024)](https://arxiv.org/pdf/2310.01558.pdf)
- [Retrieval-Generation Synergy Augmented Large Language Models (Arxiv)](https://arxiv.org/pdf/2310.05149.pdf)

---
# Fine-tune üöÄ<a name="finetune"></a>
  
## Papers


---
# Agent üöÄ<a name="agent"></a>
  
## Papers

**A survey:** [The Rise and Potential of Large Language Model Based Agents: A Survey (Arxiv)](https://arxiv.org/pdf/2309.07864.pdf) **A nice survey for LLM Agent**‚úÖüö©


